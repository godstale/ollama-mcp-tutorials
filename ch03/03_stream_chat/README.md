# Stream Chat

스트리밍을 사용한 실시간 응답 채팅 예제입니다.

## 설명

이 예제는 LangChain의 스트리밍 기능을 사용하여 AI 응답을 실시간으로 출력하는 방법을 보여줍니다. 긴 응답도 실시간으로 확인할 수 있어 더 나은 사용자 경험을 제공합니다.

## 주요 구성 요소

- **스트리밍 응답**: AI 응답을 실시간으로 출력
- **실시간 피드백**: 사용자가 응답 생성 과정을 실시간으로 확인
- **향상된 UX**: 긴 응답에 대한 즉각적인 피드백

## 사전 요구 사항

1. Ollama 설치 및 실행
2. Qwen3:8b 모델 다운로드
   ```bash
   ollama pull qwen3:8b
   ```

## 실행 방법

1. 프로젝트 디렉토리로 이동:
   ```bash
   cd ch03/03_stream_chat
   ```

2. 의존성 설치:
   ```bash
   uv sync
   ```

3. 프로그램 실행:
   ```bash
   uv run main.py
   ```

4. 질문을 입력하고 실시간으로 스트리밍되는 응답을 확인하세요.